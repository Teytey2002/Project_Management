{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import AdultDataset, BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3297 rows removed from AdultDataset.\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "# We want to difine a custom preprocesing function (custom_preprocessing(df)) from the standat dataset class \n",
    "# that will be used to transform the dataset\n",
    "\n",
    "def custom_preprocessing(df):\n",
    "    median_age = df['age'].median()\n",
    "    df['age_binary'] = df['age'].apply(lambda x: 0 if x <= median_age else 1)\n",
    "    df = df.drop('age', axis=1)\n",
    "    df['race'] = df['race'].apply(lambda x: 1 if x ==\"White\"  else 0)\n",
    "    df['sex'] =df['sex'].apply(lambda x: 1 if x ==\"Male\"  else 0)\n",
    "    return df\n",
    "# So what we did is to add a new column 'age_binary' to the dataset and drop the 'age' column, in order to \n",
    "# binarise the age column.\n",
    "# Load the dataset with the library aif360\n",
    "dataset= AdultDataset(custom_preprocessing=custom_preprocessing,\n",
    "                          protected_attribute_names=['age_binary', 'sex'], # race will remain because in the original library is defined with this protecte attribute\n",
    "                          privileged_classes=[np.array([1.0]),np.array([1.0]) ]) # We supposed that the privileged class is the old white male. It's also defined like this in the original library\n",
    "\n",
    "dataset_orig_train, dataset_orig_vt = dataset.split([0.7], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28963, 98)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age_binary', 'sex']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['education-num', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'age_binary', 'workclass=Federal-gov', 'workclass=Local-gov', 'workclass=Private', 'workclass=Self-emp-inc', 'workclass=Self-emp-not-inc', 'workclass=State-gov', 'workclass=Without-pay', 'education=10th', 'education=11th', 'education=12th', 'education=1st-4th', 'education=5th-6th', 'education=7th-8th', 'education=9th', 'education=Assoc-acdm', 'education=Assoc-voc', 'education=Bachelors', 'education=Doctorate', 'education=HS-grad', 'education=Masters', 'education=Preschool', 'education=Prof-school', 'education=Some-college', 'marital-status=Divorced', 'marital-status=Married-AF-spouse', 'marital-status=Married-civ-spouse', 'marital-status=Married-spouse-absent', 'marital-status=Never-married', 'marital-status=Separated', 'marital-status=Widowed', 'occupation=Adm-clerical', 'occupation=Armed-Forces', 'occupation=Craft-repair', 'occupation=Exec-managerial', 'occupation=Farming-fishing', 'occupation=Handlers-cleaners', 'occupation=Machine-op-inspct', 'occupation=Other-service', 'occupation=Priv-house-serv', 'occupation=Prof-specialty', 'occupation=Protective-serv', 'occupation=Sales', 'occupation=Tech-support', 'occupation=Transport-moving', 'relationship=Husband', 'relationship=Not-in-family', 'relationship=Other-relative', 'relationship=Own-child', 'relationship=Unmarried', 'relationship=Wife', 'native-country=Cambodia', 'native-country=Canada', 'native-country=China', 'native-country=Columbia', 'native-country=Cuba', 'native-country=Dominican-Republic', 'native-country=Ecuador', 'native-country=El-Salvador', 'native-country=England', 'native-country=France', 'native-country=Germany', 'native-country=Greece', 'native-country=Guatemala', 'native-country=Haiti', 'native-country=Holand-Netherlands', 'native-country=Honduras', 'native-country=Hong', 'native-country=Hungary', 'native-country=India', 'native-country=Iran', 'native-country=Ireland', 'native-country=Italy', 'native-country=Jamaica', 'native-country=Japan', 'native-country=Laos', 'native-country=Mexico', 'native-country=Nicaragua', 'native-country=Outlying-US(Guam-USVI-etc)', 'native-country=Peru', 'native-country=Philippines', 'native-country=Poland', 'native-country=Portugal', 'native-country=Puerto-Rico', 'native-country=Scotland', 'native-country=South', 'native-country=Taiwan', 'native-country=Thailand', 'native-country=Trinadad&Tobago', 'native-country=United-States', 'native-country=Vietnam', 'native-country=Yugoslavia']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 Compute fairness metric on original training dataset\n",
    "The fairness metric is Statistical Parity Difference whitch measures the disparity in positive outcomes between unprivileged and privileged groups. It compares the probability of receiving a positive outcome for members of the unprivileged group against that for members of the privileged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the privileged and unprivileged groups in order to compute the disparate impact\n",
    "privileged_groups = [{'age_binary': 1, 'sex': 1}]  # Old males\n",
    "unprivileged_groups = [{'age_binary': 0, 'sex': 0}]  # Young females\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen from the privious cell, we conclude that there is a bias in this dataset because the statistical paity metric is not equal to zero. More specifficaly, in the unprivileged group we have 2% of peopele are suffering of unfairness.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 4 Mitigate bias by transforming the original dataset via technique to ensure the classifier is fair. Here we want to use the Pre-Processing method Reweighting for fairness. This method will simply assigns weights to samples to balance the representation of protected groups in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.363877\n"
     ]
    }
   ],
   "source": [
    "# Compute the fairness metric statistical parity measure, which is the difference in the mean prediction between the unprivileged and privileged groups.\n",
    "# A negative value indicates less favorable outcomes for the unprivileged groups. in order to see if the dataset is biased\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 first instance weights originally:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The 50 first instance weights originally:')\n",
    "dataset.instance_weights[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see from the cell above that the reweight method of transforming the dataset into a fair one worked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 Compute fairness metric on transformed dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "RW.fit(dataset_orig_train)\n",
    "dataset_transf_train = RW.transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 50 first instance weights after reweighing:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.81322816, 1.        , 1.33931661, 1.        ,\n",
       "       0.56376904, 0.81322816, 1.        , 0.56376904, 1.33931661,\n",
       "       0.81322816, 1.33931661, 1.33931661, 1.        , 1.        ,\n",
       "       1.33931661, 0.81322816, 1.        , 1.        , 0.56376904,\n",
       "       1.        , 1.33931661, 1.        , 1.33931661, 1.        ,\n",
       "       1.33931661, 1.        , 1.        , 1.        , 1.33931661,\n",
       "       1.33931661, 1.        , 0.81322816, 1.        , 1.        ,\n",
       "       0.81322816, 0.56376904, 0.81322816, 0.56376904, 1.        ,\n",
       "       1.        , 1.33931661, 0.56376904, 0.56376904, 1.        ,\n",
       "       0.56376904, 0.56376904, 1.        , 0.81322816, 1.33931661])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The 50 first instance weights after reweighing:')\n",
    "dataset_transf_train.instance_weights[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for the original training set\n",
    "X_train = dataset_orig_train.features\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "X_valid = dataset_orig_valid.features\n",
    "y_valid = dataset_orig_valid.labels.ravel()\n",
    "\n",
    "X_test = dataset_orig_test.features\n",
    "y_test = dataset_orig_test.labels.ravel()\n",
    "\n",
    "# Extract data for the reweighted (fair) training set\n",
    "X_train_transf = dataset_transf_train.features\n",
    "y_train_transf = dataset_transf_train.labels.ravel()\n",
    "w_train_transf = dataset_transf_train.instance_weights\n",
    "\n",
    "# Prepare a scaler to normalize features (helpful for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_transf_scaled = scaler.fit_transform(X_train_transf)\n",
    "X_valid_transf_scaled = scaler.transform(X_valid)  # validation set stays the same\n",
    "X_test_transf_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Original Classifier (No Fairness Mitigation) =====\n",
      "Accuracy on test set: 0.8501691638472693\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.93      0.90      4655\n",
      "         1.0       0.75      0.60      0.67      1552\n",
      "\n",
      "    accuracy                           0.85      6207\n",
      "   macro avg       0.81      0.77      0.79      6207\n",
      "weighted avg       0.84      0.85      0.84      6207\n",
      "\n",
      "Statistical parity difference (original): -0.3534511260648022\n",
      "Equal opportunity difference (original): -0.27641277641277634\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Model Without Fairness Mitigation\n",
    "# ===========================\n",
    "clf_orig = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "clf_orig.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_test_orig = clf_orig.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"===== Original Classifier (No Fairness Mitigation) =====\")\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred_test_orig))\n",
    "print(classification_report(y_test, y_pred_test_orig))\n",
    "\n",
    "# Compute fairness metrics on test set\n",
    "test_bld_orig = dataset_orig_test.copy(deepcopy=True)\n",
    "test_bld_orig.labels = y_pred_test_orig.reshape(-1,1)\n",
    "\n",
    "metric_test_orig = ClassificationMetric(dataset_orig_test,\n",
    "                                        test_bld_orig,\n",
    "                                        unprivileged_groups=unprivileged_groups,\n",
    "                                        privileged_groups=privileged_groups)\n",
    "print(\"Statistical parity difference (original):\", metric_test_orig.statistical_parity_difference())\n",
    "print(\"Equal opportunity difference (original):\", metric_test_orig.equal_opportunity_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fairness Mitigated Classifier (Reweighted) =====\n",
      "Accuracy on test set: 0.833252779120348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.95      0.90      4655\n",
      "         1.0       0.76      0.48      0.59      1552\n",
      "\n",
      "    accuracy                           0.83      6207\n",
      "   macro avg       0.80      0.72      0.74      6207\n",
      "weighted avg       0.83      0.83      0.82      6207\n",
      "\n",
      "Statistical parity difference (reweighted): -0.13798345585851907\n",
      "Equal opportunity difference (reweighted): 0.24415747671561622\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Model With Fairness Mitigation (Reweighted)\n",
    "# ===========================\n",
    "clf_transf = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "# Important: use the instance weights when training on the reweighted dataset\n",
    "clf_transf.fit(X_train_transf_scaled, y_train_transf, sample_weight=w_train_transf)\n",
    "\n",
    "y_pred_test_transf = clf_transf.predict(X_test_transf_scaled)\n",
    "\n",
    "print(\"\\n===== Fairness Mitigated Classifier (Reweighted) =====\")\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred_test_transf))\n",
    "print(classification_report(y_test, y_pred_test_transf))\n",
    "\n",
    "# Compute fairness metrics on test set for the fairness mitigated classifier\n",
    "test_bld_transf = dataset_orig_test.copy(deepcopy=True)\n",
    "test_bld_transf.labels = y_pred_test_transf.reshape(-1,1)\n",
    "\n",
    "metric_test_transf = ClassificationMetric(dataset_orig_test,\n",
    "                                          test_bld_transf,\n",
    "                                          unprivileged_groups=unprivileged_groups,\n",
    "                                          privileged_groups=privileged_groups)\n",
    "print(\"Statistical parity difference (reweighted):\", metric_test_transf.statistical_parity_difference())\n",
    "print(\"Equal opportunity difference (reweighted):\", metric_test_transf.equal_opportunity_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Results:\n",
      "Original vs Reweighted:\n",
      " - Test Accuracy: 0.8502 vs 0.8333\n",
      " - Statistical Parity Difference: -0.3535 vs -0.1380\n",
      " - Equal Opportunity Difference: -0.2764 vs 0.2442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Comparison & Conclusions\n",
    "# ===========================\n",
    "print(\"\\nComparison of Results:\")\n",
    "print(\"Original vs Reweighted:\")\n",
    "print(\" - Test Accuracy: {:.4f} vs {:.4f}\".format(accuracy_score(y_test, y_pred_test_orig), accuracy_score(y_test, y_pred_test_transf)))\n",
    "print(\" - Statistical Parity Difference: {:.4f} vs {:.4f}\".format(metric_test_orig.statistical_parity_difference(),\n",
    "                                                                  metric_test_transf.statistical_parity_difference()))\n",
    "print(\" - Equal Opportunity Difference: {:.4f} vs {:.4f}\".format(metric_test_orig.equal_opportunity_difference(),\n",
    "                                                                 metric_test_transf.equal_opportunity_difference()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnixai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
